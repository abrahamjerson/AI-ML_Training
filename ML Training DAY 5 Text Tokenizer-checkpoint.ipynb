{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tokenization is the act Of breaking up a sequence Of strings into pieces such as words, keywords, \n",
    "phrases, symbols and other elements called tokens. Tokens can be individual words, phrases or even \n",
    "whole sentences. In the process of tokenization, some characters like punctuation marks are discarded. \n",
    "Word Count with CountVectorizer. The CountVectorizer provides simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary. \n",
    "\"\"\"\n",
    "\n",
    "#vectorizer is CountVectorizer class\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"This is a doxument. This came through mail. It is a prime example of document tokenizer.\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t2\n",
      "  (0, 4)\t2\n",
      "  (0, 2)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "#The transform() function is needed to encode the document as a vector.\n",
    "vector_x = vectorizer.fit_transform(corpus)\n",
    "print(vector_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
